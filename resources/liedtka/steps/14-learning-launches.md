---
title: "Step 14: Run Your Learning Launches"
source_pages: [34-35]
phase: "What Works"
type: step

retrieval_triggers:
  - "how do I test in the real world"
  - "learning launch"
  - "pilot test"
  - "market test"
  - "live experiment"
  - "testing assumptions in market"
  - "real world validation"
  - "minimum viable product"
  - "proof of concept test"
  - "live testing"

situations:
  - Ready to test concepts with real stakes
  - Co-creation went well and need next level validation
  - Need behavioral data (not just opinions)
  - Want to learn before full rollout
  - Stakeholders want proof before investing

outputs:
  - Behavioral data from real-world test
  - Validated or invalidated assumptions
  - Evidence for investment decisions
  - Learnings for concept refinement

related_steps: [13, 15]
related_tools: []
prerequisite_for: [15]
hds_workflow_phases: [13]
---

# Step 14: Run Your Learning Launches

Learning launches are experiments conducted in the real world quickly and inexpensively. They form a bridge between co-creation and full rollout. Success is measured not by how much you sell but by how much you learn.

## Core Concept

Merely asking stakeholders what they think is a weak form of testing. Learning launches ask them to put some skin in the game. Unlike co-creation sessions, learning launches need to feel real to both launchers and stakeholders.

### Learning Launch vs. Pilot
- **Pilot**: Intent is to succeed at scale
- **Learning Launch**: Intent is to learn quickly, tightly constrained during execution but open to major changes after

## The Seven Success Principles

### 1. Set Tight Boundaries
Plan for the launch to end. Set concrete limits on:
- Time
- Geography
- Number of stakeholders
- Features
- Partners

### 2. Focus on Key Assumptions
Design with sharp focus on critical assumptions that need testing. Pay special attention to "make or break" assumptions where you can learn quickly and cheaply.

### 3. Be Explicit About Data
Specify how you'll generate the data you need, especially behavioral data. Be explicit about searching for disconfirming data.

### 4. Build the Right Team
Include skeptics to ensure you aren't designing tests to give you the answers you want. Learning launches are a team sport.

### 5. Think Fast and Cheap
The learning launch is when your project first makes contact with reality. Expect surprises and be prepared to respond quickly.

### 6. Make It Feel Real
Everyone has something at stake. If your test feels like make-believe, the behavioral data it generates is suspect.

### 7. Consider a Series
Once you gather data, review and move to another round. Or, if concept hasn't met "make or break" criteria, table it.

## Learning Launch Design Template

For each learning launch, specify:

| Element | Details |
|---------|---------|
| **Key Assumptions to Test** | Which specific assumptions are you validating? |
| **Who** | Target stakeholders (actual names if possible) |
| **Where** | Location/context for the test |
| **How** | How will you make it feel real? |
| **Cost** | Budget for the learning launch |
| **Time** | Timeline with clear end date |

## Visual: Learning Launch Design Template

**Page:** 35
**Type:** template
**Recreation Priority:** HIGH

### Visual Description
A two-part template. Part 1: Fields for Key Assumptions, Who, Where, How, Cost, Time. Part 2: "What to Watch For" table with columns for Untested Assumptions, Success Metric, and Disconfirming Data.

### Key Elements
- Design section with six planning fields
- "What to Watch For" tracking table
- Space for multiple assumptions
- Clear distinction between design and measurement

### Semantic Tags for This Visual
retrieval_triggers:
  - "learning launch template"
  - "how to design a learning launch"
  - "market experiment template"

situations:
  - Planning a real-world concept test
  - Documenting learning launch parameters

### Recreation Notes
High priority. This is the key planning tool for validation. Consider making it a fillable/digital form.

## What to Watch For

Before launching, document what data would validate or disprove each assumption:

| Untested Assumption | Success Metric | Disconfirming Data |
|---------------------|----------------|-------------------|
| [Assumption] | [What would prove it true] | [What would prove it false] |

This helps ensure you don't misinterpret findings.

## How to Use This Step

1. **Select assumptions to test** — focus on "make or break" items
2. **Design the launch** using the template
3. **Document success metrics** and disconfirming data in advance
4. **Execute the launch** with tight boundaries
5. **Gather behavioral data** (actions, not just opinions)
6. **Analyze honestly** — look for disconfirming evidence
7. **Decide next steps** — iterate, pivot, or proceed

## Typical Learning Launch Progression

| Stage | Description | Time | Cost |
|-------|-------------|------|------|
| 2D paper prototype | Co-creation with 12 users | 2 weeks | $500 |
| 3D digital storyboard | Co-creation with 12 users | 6 weeks | $5,000 |
| 4D alpha test | Working prototype, 20-50 live users, 1 month | 10 weeks | $50,000 |
| Beta test | Scalable version, 100-500 users, 3 months | 20 weeks | $250,000 |

Each stage tests increasingly risky assumptions with increasing investment.

## Signs You Need This Step

- Co-creation feedback is positive but you need behavioral proof
- Stakeholders want evidence before committing resources
- You need to test assumptions about actual behavior (not stated preference)
- Ready to bridge from prototype to implementation

## Common Mistakes

### Calling It a Pilot
Pilots are meant to succeed. Learning launches are meant to learn. Different mindset.

### No Clear End Date
Without boundaries, launches expand and become mini-implementations.

### Only Looking for Good News
Confirmation bias is real. Actively seek disconfirming data.

### Too Much Investment
Keep it fast and cheap. Expensive launches are hard to kill.

### Not Making It Real
If there's no real stake, behavioral data is unreliable.

## Output

- Behavioral data validating or invalidating key assumptions
- Evidence to support investment decisions
- Learnings for next iteration
- Readiness for Step 15: Design the On-Ramp

---

*Source: Liedtka et al. (2014), The Designing for Growth Field Book, pp. 34-35*
